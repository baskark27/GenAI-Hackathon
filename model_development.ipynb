{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hackathon Level-2 Submission: Personalized Product Recommendations Using GEN AI\n",
    "\n",
    "## Objective:\n",
    "The objective of this project is to implement a personalized product recommendation system using a Generative-AI Model in Jupyter Notebook or Google Colab.\n",
    "\n",
    "## Index:\n",
    "1. Import Libraries\n",
    "2. Load Data\n",
    "3. Data Preprocessing\n",
    "4. Handle Class Imbalance\n",
    "5. Choose and Define the Model\n",
    "6. Train the Model\n",
    "7. Evaluate the Model\n",
    "8. Visualize the Results\n",
    "9. Save the Model\n",
    "\n",
    "\n",
    "\n",
    "## Setup and Implementation Steps:\n",
    "\n",
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from imblearn.over_sampling import RandomOverSampler  # Handle class imbalance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('ecommerce_data.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(data.head())\n",
    "\n",
    "# Get dataset summary\n",
    "print(data.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Fill or drop missing values\n",
    "data.fillna('', inplace=True)\n",
    "\n",
    "# Example for text data preprocessing\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Assuming 'product' column is the text column you want to clean\n",
    "data['cleaned_text'] = data['product'].apply(clean_text)\n",
    "\n",
    "# Convert categorical data to numerical\n",
    "categorical_columns = ['user id', 'product id']\n",
    "data = pd.get_dummies(data, columns=categorical_columns)\n",
    "\n",
    "# Convert 'rating' to binary target\n",
    "data['binary_rating'] = (data['rating'] >= 1000).astype(int)\n",
    "\n",
    "# Check class distribution\n",
    "print(data['binary_rating'].value_counts())\n",
    "\n",
    "X = data.drop(['rating', 'product', 'cleaned_text', 'binary_rating'], axis=1)\n",
    "y = data['binary_rating']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle class imbalance using RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_bal, y_train_bal = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the new class distribution\n",
    "print(pd.Series(y_train_bal).value_counts())\n",
    "\n",
    "# Dummy variables\n",
    "vocab_size = 5000\n",
    "embedding_dim = 128\n",
    "max_length = 100\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    LSTM(128),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_bal, y_train_bal, epochs=50, batch_size=16, validation_split=0.2)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Adjust the threshold\n",
    "threshold = 0.5  # Start with default, then adjust if needed\n",
    "predictions = (predictions > threshold).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "model.save('product_recommendation_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
